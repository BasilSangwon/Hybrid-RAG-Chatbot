import os
import time
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_neo4j import Neo4jGraph
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_google_genai import ChatGoogleGenerativeAI
# OpenAI ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ
# from langchain_openai import ChatOpenAI 

from server.core.config import NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD, GOOGLE_API_KEY, RAW_DATA_DIR

def run_graph_ingest(model_name: str, experiment_id: int, chunk_size: int = 2000, overlap: int = 200, reset_db: bool = False):
    print(f"\nğŸ•¸ï¸  [Graph Ingest] Start setup... Model: [{model_name}] | Exp ID: {experiment_id} | Chunk: {chunk_size} | Overlap: {overlap} | Reset: {reset_db}")

    # 1. Connect Neo4j
    try:
        graph = Neo4jGraph(
            url=NEO4J_URI,
            username=NEO4J_USERNAME,
            password=NEO4J_PASSWORD
        )
        print("   âœ… Neo4j Connected!")
    except Exception as e:
        print(f"   âŒ Neo4j Connection Failed: {e}")
        return

    # 2. Reset DB (ì´ˆê¸°í™” ì˜µì…˜)
    if reset_db:
        print("   ğŸ§¹ Clearing existing Neo4j data (Reset Mode)...")
        try:
            graph.query("MATCH (n) DETACH DELETE n")
            print("   âœ… DB Fully Cleared.")
        except Exception as e:
            print(f"   âš ï¸ DB Clear Failed: {e}")

    # 3. Prepare LLM (Dynamic Instantiation)
    llm = None
    if "gemini" in model_name.lower():
        llm = ChatGoogleGenerativeAI(
            model=model_name, 
            temperature=0,
            google_api_key=GOOGLE_API_KEY
        )
    elif "gpt" in model_name.lower():
        # OpenAI ì‚¬ìš© ì‹œ
        # llm = ChatOpenAI(model=model_name, temperature=0)
        print(f"   âš ï¸ OpenAI model selected ({model_name}). Make sure API key is set.")
        pass # ì‹¤ì œ êµ¬í˜„ ì‹œ ì£¼ì„ í•´ì œ
    else:
        print(f"   âš ï¸ Unknown model '{model_name}', using default Gemini Flash.")
        llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0, google_api_key=GOOGLE_API_KEY)
    
    # ---------------------------------------------------------
    # [í•µì‹¬ ìˆ˜ì •] ìŠ¤í‚¤ë§ˆ(Schema) ê°•ì œ ì •ì˜ (Schema Enforcement)
    # ---------------------------------------------------------
    # LLMì´ 'ìƒì„±', 'WITH' ê°™ì€ ì“¸ëª¨ì—†ëŠ” ê´€ê³„ë¥¼ ë§Œë“¤ì§€ ëª»í•˜ê²Œ ë§‰ê³ ,
    # ê²€ìƒ‰ í”„ë¡¬í”„íŠ¸(main.py)ì™€ ì¼ì¹˜í•˜ëŠ” êµ¬ì¡°ë¡œë§Œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê²Œ í•©ë‹ˆë‹¤.
    
    allowed_nodes = [
        "Product",      # ì œí’ˆ (Galaxy S25)
        "Feature",      # ê¸°ëŠ¥ (ì‹¤ì‹œê°„ í†µì—­)
        "Spec",         # ìŠ¤í™ (4000mAh)
        "Requirement",  # í•„ìš”ì¡°ê±´ (ë„¤íŠ¸ì›Œí¬, ê³„ì •)
        "Component",    # êµ¬ì„±ìš”ì†Œ (ì¹´ë©”ë¼, ë°°í„°ë¦¬)
        "UserManual",   # ë§¤ë‰´ì–¼ ë¬¸ì„œ
        "Section"       # ë§¤ë‰´ì–¼ ì„¹ì…˜
    ]
    
    allowed_rels = [
        "HAS_FEATURE",      # ì œí’ˆ -> ê¸°ëŠ¥
        "HAS_SPEC",         # ì œí’ˆ -> ìŠ¤í™
        "REQUIRES",         # ê¸°ëŠ¥ -> ì¡°ê±´ (ë„¤íŠ¸ì›Œí¬ ë“±)
        "INCLUDES",         # í¬í•¨ ê´€ê³„
        "PART_OF",          # êµ¬ì„± ê´€ê³„
        "RELATED_TO",       # ì¼ë°˜ì ì¸ ê´€ë ¨ì„±
        "HAS_MANUAL",       # ì œí’ˆ -> ë§¤ë‰´ì–¼
        "HAS_SECTION"       # ë§¤ë‰´ì–¼ -> ì„¹ì…˜
    ]

    llm_transformer = LLMGraphTransformer(
        llm=llm,
        allowed_nodes=allowed_nodes,
        allowed_relationships=allowed_rels,
        # node_properties=["id"] # id ì†ì„±ì€ ê¸°ë³¸ì ìœ¼ë¡œ ìƒì„±ë¨
    )
    # ---------------------------------------------------------

    # 4. Load Files
    if not os.path.exists(RAW_DATA_DIR):
        print("   âŒ Data directory not found.")
        return

    files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith('.pdf')]
    if not files:
        print("   âŒ No PDF files found.")
        return

    # 5. Processing Loop
    for filename in files:
        print(f"\nğŸ“„ Processing '{filename}' using {model_name}... (Chunk: {chunk_size})")
        file_path = os.path.join(RAW_DATA_DIR, filename)
        
        loader = PyMuPDFLoader(file_path)
        raw_docs = loader.load()
        
        # GraphëŠ” ë¬¸ë§¥ íŒŒì•…ì„ ìœ„í•´ Chunk Sizeë¥¼ ë„‰ë„‰í•˜ê²Œ ì¡ìŒ
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
        docs = text_splitter.split_documents(raw_docs)
        print(f"   -> {len(docs)} chunks created.")

        print("   â³ Extracting relationships & Tagging metadata...")
        BATCH_SIZE = 1 # API Rate Limit ê³ ë ¤
        
        for i in range(0, len(docs), BATCH_SIZE):
            batch_docs = docs[i : i + BATCH_SIZE]
            try:
                # (1) ê·¸ë˜í”„ ë¬¸ì„œ ë³€í™˜ (ìŠ¤í‚¤ë§ˆì— ë§ì¶° ì¶”ì¶œ)
                graph_docs = llm_transformer.convert_to_graph_documents(batch_docs)
                
                # (2) ë©”íƒ€ë°ì´í„° íƒœê¹… (ëª¨ë¸ëª…, íŒŒì¼ëª…)
                # ì¶”ì¶œëœ ë…¸ë“œ/ê´€ê³„ì— ì¶œì²˜ ì •ë³´ë¥¼ ê°•ì œë¡œ ì£¼ì…í•©ë‹ˆë‹¤.
                for g_doc in graph_docs:
                    for node in g_doc.nodes:
                        node.properties['source_model'] = model_name
                        node.properties['source_file'] = filename
                        if experiment_id:
                            node.properties['experiment_id'] = experiment_id
                        # idê°€ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„ (ë³´í†µì€ LLMGraphTransformerê°€ ì±„ì›Œì¤Œ)
                        if 'id' not in node.properties:
                            node.properties['id'] = node.id 

                    for rel in g_doc.relationships:
                        rel.properties['source_model'] = model_name
                        if experiment_id:
                            rel.properties['experiment_id'] = experiment_id
                
                # (3) DB ì €ì¥
                graph.add_graph_documents(graph_docs)
                print(f"      ğŸ“¦ Batch {i//BATCH_SIZE + 1} saved.")
                time.sleep(1) # íœ´ì‹ (Rate Limit ë°©ì§€)
                
            except Exception as e:
                print(f"      âš ï¸ Error in batch {i}: {e}")

    print(f"\nğŸ‰ [Success] Graph Ingestion Complete with [{model_name}]!")

# --- [2] [NEW] íŠ¹ì • ëª¨ë¸ ë°ì´í„° ì‚­ì œ í•¨ìˆ˜ ---
def delete_graph_data(model_name: str):
    """
    ì„ íƒí•œ ëª¨ë¸(source_model)ë¡œ ìƒì„±ëœ ë…¸ë“œì™€ ê´€ê³„ë§Œ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    print(f"\nğŸ—‘ï¸  [Graph Delete] Removing data for model: [{model_name}]")
    
    try:
        graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD)
        
        # í•´ë‹¹ ëª¨ë¸ íƒœê·¸ê°€ ë¶™ì€ ë…¸ë“œì™€ ê´€ê³„ë¥¼ ëª¨ë‘ ì‚­ì œí•˜ëŠ” Cypher ì¿¼ë¦¬
        query = f"""
        MATCH (n)
        WHERE n.source_model = '{model_name}'
        DETACH DELETE n
        """
        graph.query(query)
        print(f"   âœ… Successfully deleted nodes/rels for '{model_name}'")
        return True
    except Exception as e:
        print(f"   âŒ Delete Failed: {e}")
        return False

if __name__ == "__main__":
    # Test run
    run_graph_ingest(model_name="gemini-2.0-flash", experiment_id=0)
import os
import time
import pymupdf4llm
from langchain_core.documents import Document
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_neo4j import Neo4jGraph
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_google_genai import ChatGoogleGenerativeAI
# OpenAI ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ
# from langchain_openai import ChatOpenAI 

from server.core.config import NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD, GOOGLE_API_KEY, RAW_DATA_DIR

def run_graph_ingest(model_name: str, experiment_id: int, chunk_size: int = 2000, overlap: int = 200, reset_db: bool = False):
    print(f"\nğŸ•¸ï¸  [Graph Ingest] Start setup... Model: [{model_name}] | Exp ID: {experiment_id} | Chunk: {chunk_size} | Overlap: {overlap} | Reset: {reset_db}")

    # 1. Connect Neo4j
    try:
        graph = Neo4jGraph(
            url=NEO4J_URI,
            username=NEO4J_USERNAME,
            password=NEO4J_PASSWORD
        )
        print("   âœ… Neo4j Connected!")
    except Exception as e:
        print(f"   âŒ Neo4j Connection Failed: {e}")
        return

    # 2. Reset DB (ì´ˆê¸°í™” ì˜µì…˜)
    if reset_db:
        print("   ğŸ§¹ Clearing existing Neo4j data (Reset Mode)...")
        try:
            graph.query("MATCH (n) DETACH DELETE n")
            print("   âœ… DB Fully Cleared.")
        except Exception as e:
            print(f"   âš ï¸ DB Clear Failed: {e}")

    # 3. Prepare LLM (Dynamic Instantiation) - ì‚¬ìš©ì ì„ íƒ ì¡´ì¤‘
    llm = None
    if "gemini" in model_name.lower():
        llm = ChatGoogleGenerativeAI(
            model=model_name,  # ì‚¬ìš©ìê°€ ì„ íƒí•œ ëª¨ë¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            temperature=0,
            google_api_key=GOOGLE_API_KEY
        )
    elif "gpt" in model_name.lower():
        # OpenAI ì‚¬ìš© ì‹œ
        # llm = ChatOpenAI(model=model_name, temperature=0)
        print(f"   âš ï¸ OpenAI model selected ({model_name}). Make sure API key is set.")
        pass 
    else:
        print(f"   âš ï¸ Unknown model '{model_name}', using default Gemini Flash.")
        llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0, google_api_key=GOOGLE_API_KEY)
    
    # ---------------------------------------------------------
    # ìŠ¤í‚¤ë§ˆ(Schema) ì •ì˜
    # ---------------------------------------------------------
    allowed_nodes = [
        "Product", "Feature", "Spec", 
        "Requirement", "Constraint", "Condition", 
        "Component", "UserManual", "Section"
    ]
    
    allowed_rels = [
        "HAS_FEATURE", "HAS_SPEC", 
        "REQUIRES", "HAS_CONSTRAINT", "HAS_CONDITION",
        "INCLUDES", "PART_OF", "RELATED_TO", 
        "HAS_MANUAL", "HAS_SECTION"
    ]

    llm_transformer = LLMGraphTransformer(
        llm=llm,
        allowed_nodes=allowed_nodes,
        allowed_relationships=allowed_rels,
        node_properties=["name", "description"]
    )
    # ---------------------------------------------------------

    # 4. Load Files
    # DBì— ì´ë¯¸ ì €ì¥ëœ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (main.pyì˜ stats API í™œìš© ë¡œì§ ë“±ì„ ì°¸ê³ í•˜ê±°ë‚˜ ì§ì ‘ ì¿¼ë¦¬)
    try:
        existing_files = [r['source_file'] for r in graph.query("MATCH (n) WHERE n.source_model = $model RETURN DISTINCT n.source_file as source_file", {"model": model_name})]
    except:
        existing_files = []

    for filename in files:
        # [í•µì‹¬] ì´ë¯¸ í•™ìŠµí•œ íŒŒì¼ì´ë©´ ê±´ë„ˆëœë‹ˆë‹¤! (í† í° ì ˆì•½)
        if filename in existing_files:
            print(f"â© Skipping '{filename}' (Already ingested)")
            continue
            
        print(f"\nğŸ“„ Processing '{filename}'...")

    files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith('.pdf')]
    if not files:
        print("   âŒ No PDF files found.")
        return

    # 5. Processing Loop
    for filename in files:
        print(f"\nğŸ“„ Processing '{filename}' using {model_name}... (Chunk: {chunk_size})")
        file_path = os.path.join(RAW_DATA_DIR, filename)
        
        # [NEW] PyMuPDF4LLM Markdown Conversion
        try:
            print("   ğŸ“„ Converting PDF to Markdown using pymupdf4llm...")
            md_text = pymupdf4llm.to_markdown(file_path)
            # Wrap in Document object
            raw_docs = [Document(page_content=md_text, metadata={"source": filename})]
            print("   âœ… Markdown conversion successful.")
        except Exception as e:
            print(f"   âš ï¸ Markdown conversion failed: {e}. Fallback to standard loader.")
            loader = PyMuPDFLoader(file_path)
            raw_docs = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
        docs = text_splitter.split_documents(raw_docs)
        print(f"   -> {len(docs)} chunks created.")

        print("   â³ Extracting relationships & Tagging metadata...")
        BATCH_SIZE = 1 
        
        for i in range(0, len(docs), BATCH_SIZE):
            batch_docs = docs[i : i + BATCH_SIZE]
            try:
                # (1) ê·¸ë˜í”„ ë¬¸ì„œ ë³€í™˜
                graph_docs = llm_transformer.convert_to_graph_documents(batch_docs)
                
                # (2) ë©”íƒ€ë°ì´í„° íƒœê¹…
                for g_doc in graph_docs:
                    for node in g_doc.nodes:
                        node.properties['source_model'] = model_name
                        node.properties['source_file'] = filename
                        
                        # [ìˆ˜ì •] 0ë²ˆ IDë„ ì €ì¥ë˜ë„ë¡ ì¡°ê±´ ë³€ê²½
                        if experiment_id is not None:
                            node.properties['experiment_id'] = experiment_id
                            
                        if 'name' not in node.properties:
                            node.properties['name'] = node.id 
                        
                        # [FIX] Remove 'id' property if it exists to avoid Neo4j reserved keyword error
                        if 'id' in node.properties:
                            del node.properties['id'] 

                    for rel in g_doc.relationships:
                        rel.properties['source_model'] = model_name
                        if experiment_id is not None:
                            rel.properties['experiment_id'] = experiment_id
                
                # (3) DB ì €ì¥
                graph.add_graph_documents(graph_docs)
                print(f"      ğŸ“¦ Batch {i//BATCH_SIZE + 1}/{len(docs)} saved.")
                
            except Exception as e:
                print(f"      âš ï¸ Error in batch {i}: {e}")
            
            # [í•µì‹¬ ìˆ˜ì •] ì„±ê³µí•˜ë“  ì‹¤íŒ¨í•˜ë“  ë¬´ì¡°ê±´ ëŒ€ê¸° (Rate Limit ë°©ì§€)
            # try-except ë°–ìœ¼ë¡œ ë¹¼ì„œ ì—ëŸ¬ ë°œìƒ ì‹œ ì—°ì† í˜¸ì¶œ ë°©ì§€
            finally:
                print("      â³ Waiting 21s...") 
                time.sleep(21)

    print(f"\nğŸ‰ [Success] Graph Ingestion Complete with [{model_name}]!")

# --- ì‚­ì œ í•¨ìˆ˜ëŠ” ê¸°ì¡´ ìœ ì§€ ---
def delete_graph_data(model_name: str):
    print(f"\nğŸ—‘ï¸  [Graph Delete] Removing data for model: [{model_name}]")
    try:
        graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD)
        query = f"MATCH (n) WHERE n.source_model = '{model_name}' DETACH DELETE n"
        graph.query(query)
        print(f"   âœ… Successfully deleted nodes/rels for '{model_name}'")
        return True
    except Exception as e:
        print(f"   âŒ Delete Failed: {e}")
        return False

if __name__ == "__main__":
    run_graph_ingest(model_name="gemini-2.0-flash", experiment_id=0)